\documentclass{article}

\usepackage{amsmath, amsthm, amssymb}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{natbib}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{fancyvrb}
\usepackage{enumerate}
\usepackage{relsize}
\usepackage[margin=1.5in]{geometry}

\usepackage{hyperref}

\usepackage{stefan_tex}
\graphicspath{{./figures/}}

%\numberwithin{equation}{section}

%%%%%%%%% Theorems
\theoremstyle{plain}
\newtheorem{prop}{Proposition}

\newtheorem{conj}[prop]{Conjecture}
\newtheorem{coro}[prop]{Corollary}
\newtheorem{lemm}[prop]{Lemma}
\newtheorem{theo}[prop]{Theorem}

\theoremstyle{definition}
\newtheorem{exam}{Example}
\newtheorem{defi}[exam]{Definition}

\theoremstyle{remark}
\newtheorem{comm}[prop]{Comment}
\newtheorem{rema}[prop]{Remark}

\author{}
\date{\today}
\title{Notes on Shape-Constrained Partial Identification}

\begin{document}

\maketitle

A common challenge in the biomedical and social sciences is to estimate a population mean
from a sample obtained with unknown probabilities of sample selection. This is often the case
when drawing inferences about mobile populations, such as the homeless and hospital outpatients,
as well as with hard-to-reach populations, such as injection drug users and some ethnic
minorities. In general, this problem arises when a sampling frame is unavailable or unreliable,
and when there is no or limited information about the sampling design.

In brief, the estimation problem can be formalized as follows. Let $\pp$ denote a potentially
infinite population, and let $F$ denote the cumulative distribution function of our outcome
of interest $Y$ over $\pp$. Our goal is to estimate the population mean $\mu = \EE[F]{Y}$.
To do so, we have access to a random sample $\set = \cb{Y_i}$ of size $n$ obtained via biased
sampling. Concretely, we might imagine that $\set$ was generated by first drawing a larger sample
$\set^+ = \cb{Y_i}$ of size $N$ uniformly at random from $\pp$, and then drawing $\set^+$ from $\set$
with sampling probabilities $\pi_i$. Whenever these sampling probabilities are
unequal, the $\set$-sample mean will in general be an inconsistent estimator for the population mean.

If these sampling probabilities $\pi_i$ corresponding to our $n$ sampled observations were known,
then we would have access to the following natural estimator that is consistent for $\mu$
under weak conditions (HAJEK, LEHMANN)
\begin{equation}
\label{eq:hajek}
\hmu^* = \sum_{i = 1}^n \pi_i^{-1} Y_i \, \Big/ \, \sum_{i = 1}^n \pi_i^{-1}.
\end{equation}
Here, however, we are interested in the setting where the sampling weights $\pi_i$ are unknown.

In a recent advance, A-L showed that it is possible to obtain meaningful bounds on $\mu$ if all we
have is bounds on the sampling weights $\pi_i$. Specifically, suppose that we know that
$\max\cb{\pi_i} / \min\cb{\pi} \leq \gamma$ for some constant $\gamma < \infty$. Then,
A-L show that $\ii_{AL} := [\hmu_{AL}^-,\, \hmu_{AL}^+]$ forms an asymptotically consistent
identification interval for $\mu$ (MANSKI), where
\begin{equation}
\label{eq:AL}
\hmu_{AL}^+ = \max\cb{\sum_{i = 1}^n w_i Y_i : \sum_{i = 1}^n w_i = 1, \ \frac{\max\cb{w_i}}{\min\cb{w_i}} \leq \gamma},
\end{equation}
and $\hmu_{AL}^-$ is the minimum over the same set. They also develop an efficient algorithm for
computing these bounds.

Now, while this is a creative and fertile approach that can help us get identification intervals
for $\mu$ under weak assumptions, the A-L bounds can often be unnecessarily pessimistic in many
applications. Given any weights $w_i$ used to obtain a corrected mean estimate, we
can also define an implicit distribution $\hF_w(y) := \sum_{\cb{i : Y_i \leq y}} w_i$.
And the problem with the A-L approach is that the implicit distributions underlying the
interval endpoints $\hmu_{AL}^{\pm}$ are often completely implausible in practice:
As shown by A-L, the weights induced by the optimization problem \eqref{eq:AL} correspond
to a step function depending on whether or not $Y_i$ falls below some threshold, and
so the weighted empirical distribution functions $\hF_w(y)$ have a sharp change in
slope at that threshold.

This paper studies how to use auxiliary information about the shape of the population
distribution $F$ to get shorter identification intervals for $\mu$ by ruling out ``implausible''
weightings in the optimization problem \eqref{eq:AL}. In general, the more we are willing to assume
about $F$, the shorter the resulting identification intervals. At one extreme, if we know that $F$
is Gaussian, then we can substantially shorten identification intervals, while if we make weaker
assumptions, e.g., that $F$ has a log-concave density, then we get smaller but still noticeable
improvements over $\ii_{AL}$.


\section{Identification Bounds with Parametric Constraints}

We present two different ways of incorporating shape constraints on the population distribution $F$
into the original optimization problem of A-L. Our first, direct, approach focuses on the weighted empirical
distribution function $\hF^*$ induced by the oracle H\'ajek estimator $\hmu^*$ \eqref{eq:hajek}
that has access to the the true sampling probabilities $\pi_i$:
\begin{equation}
\label{eq:oracle_distr}
\hF^*(y) = \sum_{\cb{i : Y_i \leq y}} \pi^{-1}_i \, \Big/ \, \sum_{i = 1}^n \pi^{-1}_i.
\end{equation}
As shown in the result below, any shape constraint we make on $F$ that lets us control the
behavior of $\hF^*$ induces an asymptotically consistent identification interval for
the population mean $\mu$.
We note that the resulting intervals $\ii$ can never be wider than the intervals of
A-L, because the optimization problem \eqref{eq:main} has strictly more constraints than
the original optimization problem \eqref{eq:AL}.

\begin{theo}
\label{theo:main}
Suppose that we have access to auxiliary information on $F$ that lets us construct
sets of distribution functions $\cset_{n, \, \gamma}$ with the property that
$\limn \mathbb{P}[\hF^* \in \cset_{n, \, \gamma}] = 1$.
Then, if we write
\begin{equation}
\label{eq:main}
\hmu^+ = \max\cb{\sum_{i = 1}^n w_i Y_i : \sum_{i = 1}^n w_i = 1, \ \frac{\max\cb{w_i}}{\min\cb{w_i}} \leq \gamma, \ \hF_w \in \cset_{n, \, \gamma}},
\end{equation}
and $\hmu^+$ as the minimizer in the same optimization problem,
the resulting identification interval $\ii := [\hmu^-,\, \hmu^+]$ is
valid in the sense that $\Delta(\mu, \ii) \rightarrow_p 0$, where
$\Delta(\mu, \ii)$ is the distance between $\mu$ and the nearest point in the interval $\ii$.
\end{theo}

At a high level, this result shows that if we have any auxiliary information about $F$, then
the identification bounds of A-L are needlessly long.
However, the above result is of course rather abstract, and cannot directly guide practical data
analysis. First of all, it leaves open the problem of how to turn shape constraints on $F$
into plausibility sets $\cset_{\gamma, \, n}$ that contain $\hF^*$ with high probability.
Second, any guarantee of the above form is not useful if we cannot solve the optimization
problem \eqref{eq:main} in practice.
Our next concern is to address these issues given specific side-information about $F$.

\subsection{An Explicit Algorithm}

Below, we sketch a concrete algorithm that could be used with any parametric family.
We'll mostly consider just the Gaussian case; but the same algorithm would work for
any finite-dimensional family that allows for a grid-search over parameter space.

The argument should go something like this:
\begin{itemize}
\item Get a Kolmogorov-Smirnov type bound for $\hF^*$: For any $\alpha > 0$,
with probability at least $\alpha$,
$$ \limsup \sqrt{n} \sup_{y} \abs{\hF^*(y) - F(y)} \leq a(\gamma) D_{KS}(\alpha). $$
Here, $D_{KS}$ is the KS-test statistic, and TODO say what $a(\gamma)$ is.
\item We can then construct a natural plausibility set
$$ \cset_{\gamma, \, n} = \cb{\hF : \sup_{y} \abs{\hF(y) - G(y)} \leq \rho_{\gamma,\, n}, \ G \text{ is Gaussian}}, $$
where $\rho_{\gamma, \, n} = a(\gamma) D_{KS}(1/\sqrt{n})/\sqrt{n}$.
\item We can run the optimization problem just like before. We run a grid search over all
possible Gaussian (or other parametric) distributions.
\end{itemize}

\begin{coro}
If $F$ is Gaussian, then the interval $[\hmu^-, \, \hmu^+]$ is an asymptotically valid identification interval.
\end{coro}

{\bf NB:} I think that, in general, the algorithm here will also be asymptotically sharp.

\section{Identification Bounds with Non-Parametric Constraints}

Now, we can't really use the above algorithm anymore, because doing a ``grid search''
over a non-parametric class wouldn't be practical.

Here's a concrete algorithm that can be used to leverage the fact that $F$ is log-concave
to get shorter identification bounds than A-L. These bounds probably aren't 
sharp, even asymptotically ... but at least they enable the statistician to make
some use of the shape constraints!

This is a derivation for the upper endpoint of the interval.
To get the lower end point, we apply the whole same argument to $-Y_i$.
\begin{itemize}
\item First, write $S(y)$ for the cumulative distribution function of the
\emph{sampling} distribution, and write $\hS(y)$ for the empirical distribution
function of the observed sample.
\item Use a vanilla KS-type argument to get a lower bound $S(y) \geq \hS^+(y) := \hS(y) - \delta$
uniformly for all $y$.
\item Next, we need to down-weight this lower bound. We know from A-L that all the
worst case weightings are step functions for some threshold $t$. For any such threshold
$t$, we can get a lower bound $\hH_t(\cdot)$ by down-weighting $\hS^+(\cdot)$ using
these weights. By the argument of A-L, we know that there exists some threshold $t$
for which $F \geq \hH_t$, with high probability.
\item Now, we also know that that $F$ is log-concave, which also means that the function
$F$ itself is log-concave. (Note: this is not an if-and-only-if relation, so we are relaxing
a little here.) Now, if $F \geq \hH_t$ and $F$ is log-concave, then $F \geq \hL_t$,
where $\hL_t$ is the log-concave envelope of $\hH_t$. (Specifically, just make a picture
of $(y, \, \log(\hH_t(y)))$, and then take the convex hull of these points. The ``top side''
of this convex hull is $\hL_t$).
\item Define the infinimum of the lower bounds as $\hL(y) = \inf_t \hL_t(y)$.
Clearly, $F \geq \hL$ with high probability. 
\item Our set is now
$$ \cset^+_{\gamma, \, n} = \cb{\hF : \hF(y) \geq \hL(y) \text{ for all } y \in \RR}. $$
We use this in \eqref{eq:main} to get $\hmu^+$.
\end{itemize}

This algorithm does not quite fall under the scope of Theorem \ref{theo:main}, because we
cannot guarantee that $\hF^* \in \cset^+$. But the result we want still holds.

\begin{theo}
If $F$ is log-concave, then the interval $[\hmu^-, \, \hmu^+]$ is an asymptotically valid identification interval.
\end{theo}

TODO: make the presentation more general. I.e., like in the previous section, first state
a general result, and then make the log-concave application be a corollary.

\end{document}