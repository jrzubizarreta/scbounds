\documentclass{article}

\usepackage{amsmath, amsthm, amssymb}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{natbib}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{fancyvrb}
\usepackage{enumerate}
\usepackage{relsize}
\usepackage[margin=1.5in]{geometry}

\usepackage{hyperref}

\usepackage{stefan_tex}
\graphicspath{{./figures/}}

%\numberwithin{equation}{section}

%%%%%%%%% Theorems
\theoremstyle{plain}
\newtheorem{prop}{Proposition}

\newtheorem{conj}[prop]{Conjecture}
\newtheorem{coro}[prop]{Corollary}
\newtheorem{lemm}[prop]{Lemma}
\newtheorem{theo}[prop]{Theorem}

\theoremstyle{definition}
\newtheorem{exam}{Example}
\newtheorem{defi}[exam]{Definition}

\theoremstyle{remark}
\newtheorem{comm}[prop]{Comment}
\newtheorem{rema}[prop]{Remark}

\author{}
\date{\today}
\title{Notes on Shape-Constrained Partial Identification}

\begin{document}

\maketitle

A common challenge in the biomedical and social sciences is to estimate a population mean
from a sample obtained with unknown probabilities of sample selection. This is often the case
when drawing inferences about mobile populations, such as the homeless and hospital outpatients,
as well as with hard-to-reach populations, such as injection drug users and some ethnic
minorities. In general, this problem arises when a sampling frame is unavailable or unreliable,
and when there is no or limited information about the sampling design.

In brief, the estimation problem can be formalized as follows. Let $\pp$ denote a potentially
infinite population, and let $F$ denote the cumulative distribution function of our outcome
of interest $Y$ over $\pp$. Our goal is to estimate the population mean $\mu = \EE[F]{Y}$.
To do so, we have access to a random sample $\set = \cb{Y_i}$ of size $n$ obtained via biased
sampling. Concretely, we might imagine that $\set$ was generated by first drawing a larger sample
$\set^+ = \cb{Y_i}$ of size $N$ uniformly at random from $\pp$, and then drawing $\set^+$ from $\set$
with sampling probabilities $\pi_i$. Whenever these sampling probabilities are
unequal, the $\set$-sample mean will in general be an inconsistent estimator for the population mean.

If these sampling probabilities $\pi_i$ corresponding to our $n$ sampled observations were known,
then we would have access to the following natural estimator that is consistent for $\mu$
under weak conditions (HAJEK, LEHMANN)
\begin{equation}
\label{eq:hajek}
\hmu^* = \sum_{i = 1}^n \pi_i^{-1} Y_i \, \Big/ \, \sum_{i = 1}^n \pi_i^{-1}.
\end{equation}
Here, however, we are interested in the setting where the sampling weights $\pi_i$ are unknown.

In a recent advance, A-L showed that it is possible to obtain meaningful bounds on $\mu$ if all we
have is bounds on the sampling weights $\pi_i$. Specifically, suppose that we know that
$\max\cb{\pi_i} / \min\cb{\pi} \leq \gamma$ for some constant $\gamma < \infty$. Then,
A-L show that $\ii_{AL} := [\hmu_{AL}^-,\, \hmu_{AL}^+]$ forms an asymptotically consistent
identification interval for $\mu$ (MANSKI), where
\begin{equation}
\label{eq:AL}
\hmu_{AL}^+ = \max\cb{\sum_{i = 1}^n w_i Y_i : \sum_{i = 1}^n w_i = 1, \ \frac{\max\cb{w_i}}{\min\cb{w_i}} \leq \gamma},
\end{equation}
and $\hmu_{AL}^-$ is the minimum over the same set. They also develop an efficient algorithm for
computing these bounds.

Now, while this is a creative and fertile approach that can help us get identification intervals
for $\mu$ under weak assumptions, the A-L bounds can often be unnecessarily pessimistic in many
applications. Given any weights $w_i$ used to obtain a corrected mean estimate, we
can also define an implicit distribution $\hF_w(y) := \sum_{\cb{i : Y_i \leq y}} w_i$.
And the problem with the A-L approach is that the implicit distributions underlying the
interval endpoints $\hmu_{AL}^{\pm}$ are often completely implausible in practice:
As shown by A-L, the weights induced by the optimization problem \eqref{eq:AL} correspond
to a step function depending on whether or not $Y_i$ falls below some threshold, and
so the weighted empirical distribution functions $\hF_w(y)$ have a sharp change in
slope at that threshold.

This paper studies how to use auxiliary information about the shape of the population
distribution $F$ to get shorter identification intervals for $\mu$ by ruling out ``implausible''
weightings in the optimization problem \eqref{eq:AL}. In general, the more we are willing to assume
about $F$, the shorter the resulting identification intervals. At one extreme, if we know that $F$
is Gaussian, then we can substantially shorten identification intervals, while if we make weaker
assumptions, e.g., that $F$ has a log-concave density, then we get smaller but still noticeable
improvements over $\ii_{AL}$.

\section{Identification Bounds under Shape Constraints}

Our goal is to use shape constraints on the population distribution $F$ to add constraints
to the original optimization problem of A-L. To do so, we focus on the weighted empirical
distribution function $\hF^*$ induced by the oracle H\'ajek estimator $\hmu^*$ \eqref{eq:hajek}
that has access to the the true sampling probabilities $\pi_i$:
\begin{equation}
\label{eq:oracle_distr}
\hF^*(y) = \sum_{\cb{i : Y_i \leq y}} \pi^{-1}_i \, \Big/ \, \sum_{i = 1}^n \pi^{-1}_i.
\end{equation}
As shown in the result below, any shape constraint we make on $F$ that lets us control the
behavior of $\hF^*$ induces an asymptotically consistent identification interval for
the population mean $\mu$.
We note that the resulting intervals $\ii$ can never be wider than the intervals of
A-L, because the optimization problem \eqref{eq:main} has strictly more constraints than
the original optimization problem \eqref{eq:AL}.

\begin{theo}
\label{theo:main}
Suppose that we have access to auxiliary information on $F$ that lets us construct
potentially random sets of distribution functions $\cset_{n, \, \gamma}$ that
depend only on the observed sample $Y_1, \, \ldots, \, Y_n$, with the property that
$\limn \mathbb{P}[\hF^* \in \cset_{n, \, \gamma}] = 1$.
Then, if we write
\begin{equation}
\label{eq:main}
\hmu^+ = \max\cb{\sum_{i = 1}^n w_i Y_i : \sum_{i = 1}^n w_i = 1, \ \frac{\max\cb{w_i}}{\min\cb{w_i}} \leq \gamma, \ \hF_w \in \cset_{n, \, \gamma}},
\end{equation}
and $\hmu^+$ as the minimizer in the same optimization problem,
the resulting identification interval $\ii := [\hmu^-,\, \hmu^+]$ is
consistent in the sense that $\mu \in \ii$ with probability tending to 1.
\end{theo}

At a high level, this result shows that if we have any auxiliary information about $F$, then
the identification bounds of A-L are needlessly long.
However, the above result is of course rather abstract, and cannot directly guide practical data
analysis. First of all, it leaves open the problem of how to turn shape constraints on $F$
into plausibility sets $\cset_{\gamma, \, n}$ that contain $\hF^*$ with high probability.
Second, any guarantee of the above form is not useful if we cannot solve the optimization
problem \eqref{eq:main} in practice.
Our next concern is to address these issues given specific side-information about $F$.
First, we consider the case where $F$ is known to have a Gaussian (or log-parabolic) density,
and, second, we study the weaker constraint by which we only know that $F$ is log-concave.

\subsection{Identification Bounds with Gaussian Constraints}

The argument should go something like this:
\begin{itemize}
\item Get a Kolmogorov-Smirnov type bound for $\hF^*$: For any $\alpha > 0$,
with probability at least $\alpha$,
$$ \limsup \sqrt{n} \sup_{y} \abs{\hF^*(y) - F(y)} \leq a(\gamma) D_{KS}(\alpha). $$
Here, $D_{KS}$ is the KS-test statistic, and TODO say what $a(\gamma)$ is.
\item We can then construct a natural plausibility set
$$ \cset_{\gamma, \, n} = \cb{\hF : \sup_{y} \abs{\hF(y) - G(y)} \leq \rho_{\gamma,\, n}, \ G \text{ is Gaussian}}, $$
where $\rho_{\gamma, \, n} = a(\gamma) D_{KS}(1/\sqrt{n})/\sqrt{n}$. Note that this is
is non-random.
\item We can run the optimization problem just like before. We run a grid search over all
possible Gaussian distributions.
\end{itemize}


\begin{coro}
If $F$ is Gaussian, then the interval $[\hmu^-, \, \hmu^+]$ covers $\mu$ with probability tending to 1.
\end{coro}

\subsection{Identification Bounds with Log-Concave Constraints}

Now, the argument is a little more involved. The problem is that we can't leverage
the above estimation idea, because we can't run a grid search over all log-concave
distributions. Instead, we need to do something much more indirect.

This is a derivation for the upper endpoint of the interval.
\begin{itemize}
\item First, write $S(y)$ for the cumulative distribution function of the
\emph{sampling} distribution, and write $\hS(y)$ for the empirical distribution
function of the observed sample.
\item Use a vanilla KS-type argument to get a lower bound $S(y) \geq \hS^+(y) := \hS(y) - \delta$
uniformly for all $y$.
\item Next, we need to down-weight this lower bound. We know from A-L that all the
worst case weightings are step functions for some threshold $t$. For any such threshold
$t$, we can get a lower bound $\hH_t(\cdot)$ by down-weighting $\hS^+(\cdot)$ using
these weights. By the argument of A-L, we know that there exists some threshold $t$
for which $F \geq \hH_t$, with high probability.
\item Now, we also know that that $F$ is log-concave, which also means that the function
$F$ itself is log-concave. (Note: this is not an if-and-only-if relation, so we are relaxing
a little here.) Now, if $F \geq \hH_t$ and $F$ is log-concave, then $F \geq \hL_t$,
where $\hL_t$ is the log-concave envelope of $\hH_t$. (Specifically, just make a picture
of $(y, \, \log(\hH_t(y)))$, and then take the convex hull of these points. The ``top side''
of this convex hull is $\hL_t$.
\item Our set is now
$$ \cset^+_{\gamma, \, n} = \cb{\hF : \hF(y) \geq \min_t \hL_t(y) \text{ for all } y \in \RR}. $$
We use this in \eqref{eq:main} to get $\hmu^+$.
\end{itemize}
To get the lower end point, we apply the whole same argument to $-Y_i$.

This algorithm does not quite fall under the scope of Theorem \ref{theo:main}, because we
cannot guarantee that $\hF^* \in \cset^+$. But the result we want still holds.

\begin{theo}
If $F$ is log-concave, then the interval $[\hmu^-, \, \hmu^+]$ covers $\mu$ with probability tending to 1.
\end{theo}

\end{document}